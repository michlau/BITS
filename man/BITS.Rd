% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bits.R
\name{BITS}
\alias{BITS}
\alias{intstump}
\alias{BITS.complete}
\title{Boosting interaction tree stumps}
\usage{
BITS(
  X,
  y,
  max.vars = 3,
  gamma = NULL,
  boosting.iter = 50,
  learning.rate = 0.1,
  lambda = NULL,
  alpha = 1,
  nfolds = 0,
  nlambda = 100,
  relax = TRUE,
  gamma2 = 0.5,
  adjust.shady.int = TRUE,
  term.select = "elnet",
  negate = TRUE,
  max.iter = function(p) 4 * max.vars * p,
  set_vars = NULL
)

intstump(
  X,
  y,
  negate = TRUE,
  max.vars = 3,
  gamma = NULL,
  max.iter = function(p) 4 * max.vars * p,
  adjust.shady.int = TRUE
)

BITS.complete(
  X,
  y,
  max.vars = 3,
  gamma = NULL,
  boosting.iter = 50,
  learning.rate = 0.1,
  lambda = NULL,
  alpha = 1,
  nfolds = 0,
  nlambda = 100,
  relax = TRUE,
  gamma2 = 0.5,
  adjust.shady.int = TRUE,
  term.select = "elnet",
  negate = TRUE,
  max.iter = function(p) -1,
  parallel = TRUE,
  gmin = function(gmax) 0.001 * gmax,
  gsteps = 50
)
}
\arguments{
\item{X}{Matrix or data frame of \eqn{p} input variables}

\item{y}{Numeric vector of a binary or continuous outcome}

\item{max.vars}{Maximum interaction order}

\item{gamma}{Penalty value for penalizing long interaction terms}

\item{boosting.iter}{Number of boosting iterations}

\item{learning.rate}{Learning rate in gradient boosting}

\item{lambda}{Sequence of lasso/elastic net penalties for pruning the
identified terms/fitting the final model}

\item{alpha}{Elastic net balancing parameter between the lasso and the ridge
penalty}

\item{nfolds}{Number of cross-validation folds if the final lasso/elastic net
shall be fitted using cross-validation}

\item{nlambda}{Number of considered lasso/elastic net penalty values if no
\code{lambda} sequence is provided}

\item{relax}{Shall a relaxed lasso be fitted?}

\item{gamma2}{Relaxed lasso balancing parameter between the lasso fit and
the unpenalized fit}

\item{adjust.shady.int}{Shall adjustment for shady interactions be performed?}

\item{term.select}{Type of final term selection. Either \code{"elnet"} for
using an elastic net/lasso (default) or \code{"step"} for performing a
stepwise regression.}

\item{negate}{Shall negations of input variables also be considered? Default
is \code{TRUE} and uses the maxima of each input variable for negations.
Alternatively, a numeric vector of length \eqn{p} can be supplied that
specifies the negation offsets for all input variables, i.e., that
specifies \eqn{m_j} for the negated variable \eqn{X_j^c = m_j-X_j}.}

\item{max.iter}{A function taking one argument -- the number \eqn{p} of input
variables -- that returns the maximum number of search iterations in
fitting interaction tree stumps. At least \eqn{\code{max.vars} \cdot p}
should be conducted for performing a greedy search. For performing a
complete search, a negative number should be returned by this function.
To decide if a complete search is feasible, the function
\code{\link{calcNoPossibleTerms}} can be used to determine the size of the
search space.}

\item{set_vars}{The complete search space as a C++ object. Internal argument
only used for the complete search.}

\item{parallel}{Shall parallel computation be enabled?}

\item{gmin}{Ratio of the minimum \code{gamma} value/maximum \code{gamma} value}

\item{gsteps}{Number of \code{gamma} values}
}
\value{
The function \code{BITS} returns an object of class \code{BITS}.
  This is a list containing
  \item{\code{lin.mod}}{The linear model of identified terms that was fitted
    using the method specified via \code{term.select}}
  \item{\code{disj}}{A matrix of the identified terms. Each row corresponds
    to a single term and each entry corresponds to the column index in \code{X}.
    Negative values indicate negations. Missing values mean that the term
    does not contain any more variables.}
  \item{\code{s}}{Chosen value for the lasso/elastic net term inclusion
    penalty \code{lambda}. Automatically set if cross-validation (through
    \code{nfolds}) is employed. Otherwise, should be manually set to the
    desired value after fitting the BITS model. The function
    \code{\link{get.ideal.penalty}} can be used for determining the ideal
    \code{lambda} penalty using independent validation data.}
  \item{\code{y_bin}}{Boolean whether the outcome is binary}
  \item{\code{neg.offset}}{Numeric vector of length \eqn{p} containing the
    identified or supplied negation offsets}
  \item{\code{gamma2}}{The relaxed lasso balancing parameter between the
    lasso fit and the unpenalized fit}
  \item{\code{evaluated.terms}}{Number of (completely) evaluated terms in all
    interaction tree stump fittings}
  \item{\code{possible.terms}}{Number of possible terms for fitting
    interaction tree stumps}
  \item{\code{set_vars}}{Search space as a C++ object. Only for internal
    use.}
  The function \code{BITS.complete} returns an object of class
  \code{BITS.list}, which is a list of fitted \code{BITS} models for different
  \code{gamma} values. The \code{gamma} value for each model in this list can
  be accessed via \code{$gamma}.
}
\description{
The main function that fits BITS models.
}
\details{
BITS is a statistical learning procedure that fits generalized linear models
and autonomously detects and incorporates interaction effects.
Gradient boosting is employed using interaction tree stumps as base learner.
Interaction tree stumps, i.e., decision tree stumps or simple linear
regression models that include a marginal effect or an interaction effect,
are fitted either by a complete search over all possible interaction terms
of up to maximum interaction order \code{max.vars}. If a higher-dimensional
task is considered, a hybrid between a greedy and a complete search should be
carried out limiting the maximum number of search iterations.
For controlling model complexity, long interactions are penalized using the
hyperparameter \code{gamma}.
Furthermore, the boosted model is pruned to the most important terms using
a relaxed lasso/elastic net.

The main function \code{BITS} is designed to use in conjunction with a fixed
\code{gamma} value.
Alternatively, the function \code{BITS.complete} can be used that trains
\code{BITS} models on a grid of \code{gamma} and \code{lambda} values.

\code{BITS.complete} can compute the \code{gamma}-paths in parallel.
For parallel computing, a cluster has to be registered beforehand, e.g., via
\preformatted{
 cl <- parallel::makeCluster(4) # Using 4 threads
 doParallel::registerDoParallel(cl)
}

The function \code{intstump} fits a single interaction tree stump.
}
\examples{
library(BITS)

# Generate toy data
set.seed(123)
maf <- 0.25
n <- 3000
p <- 50
X <- matrix(sample(0:2, p * n, replace = TRUE,
                   prob = c((1-maf)^2, 1-(1-maf)^2-maf^2, maf^2)), ncol = p)
truth <- X[,1] + X[,2] * (X[,3] > 0) + 2 * X[,2] * (X[,4] < 2) * (2-X[,5])
y <- truth + rnorm(n, 0, sd(truth))

train.ind <- 1:1000; val.ind <- 1001:2000; test.ind <- 2001:3000

# Identify and apply modes of inheritance
X.moi <- applyMOI(X, MOI(X[train.ind,], y[train.ind]))

# Fit BITS models for a grid of gamma and lambda values
model <- BITS.complete(X[train.ind,], y[train.ind], max.iter = function(p) 1000,
                       gsteps = 10)
# Get ideal hyperparameters using validation data
ideal.mod <- get.ideal.model(model, X[val.ind,], y[val.ind])
# Plot validation data performance
plot(ideal.mod$val.res)

# Fit the final BITS model using the optimal hyperparameters
model <- BITS(X[c(train.ind, val.ind),], y[c(train.ind, val.ind)],
              gamma = ideal.mod$best.g, max.iter = function(p) 1000)
model$s <- ideal.mod$best.s

# Normalized mean squared error
calcNMSE(predict(model, X[test.ind,]), y[test.ind])
# Terms included in the final model
get.included.vars(model)
}
\references{
\itemize{
  \item Lau, M., Schikowski, T. & Schwender, H. (2023).
  Boosting interaction tree stumps. To be submitted.
}
}
