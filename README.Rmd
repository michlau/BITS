---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# BITS

<!-- badges: start -->
[![R-CMD-check](https://github.com/michlau/BITS/workflows/R-CMD-check/badge.svg)](https://github.com/michlau/BITS/actions)
<!-- badges: end -->

BITS (boosting interaction tree stumps) is a statistical learning method that fits linear interaction models such as
$$\mathbb{E}[Y | \boldsymbol{X} = \boldsymbol{x}] = \beta_0 + \beta_1 x_1 + \beta_2 x_3 x_{42} + \beta_3 x_9 + \beta_4 x_1 x_5 x_{10}.$$
The main idea is to use interaction tree stumps, i.e., decision tree stumps or simple linear regression models that may depend on one single input feature or interaction term, as base learner in gradient boosting.

To avoid overfitting and detecting unnecessarily complex interactions, interaction tree stumps $m$ are fitted by minimizing $\mathrm{MSE}(m) + \gamma ||m||_0$, where $\gamma \geq 0$ is a penalty hyperparameter and $||m||_0$ counts the number of variables contained in the term modeled by the interaction tree stump $m$.

## Installation

You can install the latest version of BITS from GitHub like so:

``` r
devtools::install_github("michlau/BITS")
```

## Example

Here is an example of an epidemiological toy data set consisting of some SNPs and a target quantitative phenotype.
```{r}
library(BITS)
```

### Data generation
```{r}
set.seed(123)
maf <- 0.25
n <- 3000
p <- 50
X <- matrix(sample(0:2, p * n, replace = TRUE,
                   prob = c((1-maf)^2, 1-(1-maf)^2-maf^2, maf^2)), ncol = p)
truth <- X[,1] + X[,2] * (X[,3] > 0) + 2 * X[,2] * (X[,4] < 2) * (2-X[,5])
y <- truth + rnorm(n, 0, sd(truth))

train.ind <- 1:1000; val.ind <- 1001:2000; test.ind <- 2001:3000
```

### Identify and apply modes of inheritance
```{r}
X.moi <- applyMOI(X, MOI(X[train.ind,], y[train.ind]))
```

### Fit BITS models for a grid of gamma and lambda values
```{r}
model <- BITS.complete(X[train.ind,], y[train.ind], max.iter = function(p) 1000,
                       gsteps = 10)
```

### Get ideal hyperparameters using validation data
```{r}
ideal.mod <- get.ideal.model(model, X[val.ind,], y[val.ind])
```

### Plot validation data performance
```{r, echo = FALSE, fig.align='center', dpi=300}
plot(ideal.mod$val.res)
```

### Fit the final BITS model using the optimal hyperparameters
```{r}
model <- BITS(X[c(train.ind, val.ind),], y[c(train.ind, val.ind)],
              gamma = ideal.mod$best.g, max.iter = function(p) 1000)
model$s <- ideal.mod$best.s
```

### Compute the normalized mean squared error
```{r}
calcNMSE(predict(model, X[test.ind,]), y[test.ind])
```

### Show the terms included in the final model
```{r}
get.included.vars(model)
```
